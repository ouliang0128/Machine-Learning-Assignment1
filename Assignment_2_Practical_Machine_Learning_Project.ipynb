{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Assignment 2: Practical Machine Learning Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ouliang0128/Machine-Learning-Assignments/blob/master/Assignment_2_Practical_Machine_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9IrWnYCXEy5",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2: Practical Machine Learning Project\n",
        "Fraud detecion in banking buseness with multi-criteria decision making framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vwkuP8ZQrDkL"
      },
      "source": [
        "# Abstract\n",
        "Since a variety of digital transaction approaches be popular in daily life, the probability of transaction data fraud raises in the information era which leads billions of dollars of the USA to be lost. However, traditional fraud detection ways are not more suitable for the growing amount of data. For detecting the fraud data quicker and reliable, machine learning technology is applied. Although previous literature had made a great contribution to the fraud detection area by machine learning technology, there are still some limitations to their researches. The one most important issue is about multiple-criteria decision making because the facticity of the predicted result can not only be determined by accuracy, which will be solved in this report. This paper will describe how the authors mix the artificial neural network with the NSGA-II algorithm to build an MCDM framework to achieve fraud detection in the credit card data which is anonymous and provided from Kaggle and shows the tested result of the framework.\n",
        "\n",
        "Keywords: Machine learning, Neural network, Genetic, MCDM and optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCur6bZeXRXA",
        "colab_type": "text"
      },
      "source": [
        "# 1.Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBgbgksrXe5N",
        "colab_type": "text"
      },
      "source": [
        "With the development of new technology applied in the bank prevents system, criminals also renovate the way they using for fraudulent. This report proposed a new method which is different from the previous studies to detect fraud in the banking business. The approach utilized the data mining technology will make the standard of fraud detection be more accurate and speedy. Both bankers and customers could benefit from the new method. This report believes that the impact of fraud must not be neglected, although it only consists of a small proportion of the whole transactions. This is because if the fraud transaction cannot be denied in real-time, the loss of customers and the bank will be inevitable. Therefore, data mining method must be introduced to accelerate the detection speed, prevent devastating consequences from happening. This report will focus on data mining technologies and deliver a feasible and flexible solution for detecting banking fraud.\n",
        "\n",
        "Base on the literature review of previous studies, those studies have introduced a variety of solutions. Although these solutions achieve high accuracy in detecting fraud, most of them have not explored the essence of fraud, which is a class imbalance. Moreover, machine learning algorithms that those studies employed are limited, with little or without comparison among them. As a result, our research aims to avoid these disadvantages.\n",
        "\n",
        "This report is organized as the following structure. In section 2, five aims of our research are explained in details. In section 3, the background information of this research will be wide introduced basing on literature review from previous successful articles. The next section systematically describes the significance and innovation of this study. In section 5, the overall research approach is integrally analyzed and the several activities of the approach are explained. Finally, section 6 summarizes the whole content of this research proposal.\n",
        "\n",
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxZt79ooXg9f",
        "colab_type": "text"
      },
      "source": [
        "##1.1. Research Aims and Objectives \n",
        "This research aims to establish a framework that combines the neural network and NSGA-II algorithm to achieve data prediction through multi-criteria decision making. This framework is a scalable framework, which means it is available for further researchers to edit and add any fitness functions into training. With this automatic model, on the one hand, the fraud credit card transactions can be identified quickly and intelligently. On the other hand, the property of the credit card owner will be protected by this model when someone illegally uses their credit card."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf4vh6gmXb7s",
        "colab_type": "text"
      },
      "source": [
        "##1.2. Background\n",
        "\n",
        "Both individuals and banking institutions are threatened by the increasingly serious issue, fraud in the banking business. Nisbet et al. (2018, cited in Eshghi & Kargari 2019, p.382) point out that fraud happened in the USA causes over than $100 billion loss per year. From bank aspect, Transaction fraud is the most concern, especially credit card fraud. Due to the rapid raising of banking business, the job, fraud detection, becomes more challengeable and indispensable than before.\n",
        " \n",
        "Although â€˜telephone bank, online bank and mobile bank' (Guo et al. 2018), these various tools provided by bank make it easier for customers to access banking services, the potential risk of fraud is also increasing. Using correct PIN is the usual way to identify the customers' identity, however, because of the fact, the most user used to set a password which is easy to remember, or phishing, people still suffer from identity fraud every day. Therefore, verifying the authorization is from the user themselves a hard problem (Mason & Bohm 2017).\n",
        " \n",
        "Furthermore, According to the Mastercard, the loss, which reaches in $ 118 billion, caused by falsely identified fraud far more than the loss because of the actual fraud in 2014 in The US market(Kim et al. 2019). Be protected by the law, customers do need to provide the evidence to the bank that they did not get the money which does not belong to them(Kim et al. 2019). Thus, the duty of offering evidence is taken by the bank(Kim et al. 2019). Because of this, a bank should do more work on the detection job to avoid the damage from fraud.\n",
        " \n",
        "The traditional way to detect data fraud mainly relies on the artificial audit combining visualization and statistics tools(Turban, Sharda & Delen 2010). However, it is time-consuming, high-cost, and impractical to use the traditional way to detect fraud while the explosion of business data is showing up (West & Bhattacharya 2016, p. 47). \n",
        "\n",
        "Different from the traditional ways, detection using data mining technology could identify fraud fast. However, the accuracy of detection models would decrease due to the changing of fraudsters and different transaction data have different features.\n",
        "Therefore, building an adaptable data mining models is necessary for fraud detection. Ravisankar et al. (2011) successfully applied the data mining technology and feature selection algorithm named the t-statistic for fraud detection on the financial statement in the early year. However, the approach they used for feature selection is limited, because it can only pick up 10 features from 18 features and only accept consistent data format.\n",
        " \n",
        "In terms of banking business fraud detection, bank mainly focuses on transaction fraud and credit card fraud. A framework was introduced by Wei et al. (2013), which detects transaction fraud effectively. They combine the supervised data mining methods and domain knowledge, mixed features and a multiple-layer structure together based on the online transaction data. Their frame performs both efficiency and accuracy for the online transaction data, this kind of extremely imbalanced data. However, a new study points out that it is not enough for fraud detection to only use domain knowledge and supervised methods, because the fraudsters are changing too (Eshghi & Kargari 2019, p. 383)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdye0g-zgwh",
        "colab_type": "text"
      },
      "source": [
        "##1.3 Research Ethics\n",
        "\n",
        "In fraud detection research, the researcher should protect the privacy of the data owners and never reveal the data. But the data set used in this paper is provided from the Kaggle which is a public web site and all data and attributes of the data set are anonymously recorded. So it is impossible for people to extra the information of a specific owner or credit card from the data set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C5fCABQX8Xs",
        "colab_type": "text"
      },
      "source": [
        "# 2. Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBgpi-KJX_jJ",
        "colab_type": "text"
      },
      "source": [
        "# 3. Methodology\n",
        "Since this research aims to build a solution to address the fraud detection issue, the overall approach is shown in Fig.5-1, which is an incremental approach. For achieving the approach, the research team will experiment with the whole process in a laboratory environment.\n",
        "\n",
        "\n",
        "First, data is collected from the management information systems of banks. Because of the class imbalance of the dataset, special sampling data method will be applied.\n",
        "Over-sampling and under-sampling are the two common methods for dealing with class imbalance. While over-sampling simply means to collect comparatively more rows from the minority class, which often includes resampling (duplicate one record for many times), under-sampling reduces the proportion of majority class in the sample. Random under-sampling (RUS) and random over-sampling (ROS) are the two widely used forms for two of them (Johnson & Khoshgoftaar 2019).\n",
        "The next stage is feature selection, which is the first step in the typical data mining process. The feature selection will implement the smart feature translation algorithm for adapting to different formats of datasets, which guarantee the flexibility of this research. Then, MCDM framework is established and models using Machine learning technologies are built. The results from models will be validated by the MCDM framework, which yields quantitative scores for models' results. After comparing results among different models, the optimized model will be deployed and tested in real-world business. Feedback from the test will utilize to adjust this model, and the whole modeling cycle will start again. After several flexible incremental iterations, a final model can be produced.\n",
        "\n",
        "5.3.6 Validate and compare results from different models\n",
        "The testing data set will be put into several models and generated scores. By comparing scores among models (an envision is shown in Table 5-1), the best model will be identified. \n",
        "\n",
        "Medel\tFPR (%)\tTPR (%)\tAUC\tAccuracy (%)\n",
        "LACE\t67.45\t67.31\t0.535\t95.9\n",
        "ANN\t46.41\t76.55\t0.632\t89.3\n",
        "CNN\t55.72\t65.18\t0.735\t84.7\n",
        "RF\t83.55\t74.98\t0.901\t88.9\n",
        "SVM\t66.29\t56.41\t0.502\t92.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhlUAAk2WbWC",
        "colab_type": "text"
      },
      "source": [
        "# 3.1 Implement Algorithms; \n",
        "In this subsection, python code for our model is shown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YJ4ryS9WNXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCuvkF3sYAG0",
        "colab_type": "text"
      },
      "source": [
        "# 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHaWlAZ0YFrK",
        "colab_type": "text"
      },
      "source": [
        "# 5. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkw1r0JqYXF5",
        "colab_type": "text"
      },
      "source": [
        "# Video Pitch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3rdCWbMdG5D",
        "colab_type": "text"
      },
      "source": [
        "# TEST BP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR5iv9-ldAYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY4Qs7Qvc3Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, neuron_numbers_in_layers=[2, 4, 1]):\n",
        "        super(MyNN, self).__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.Linear(in_features=nin, out_features=nout)\n",
        "             for nin, nout in zip(neuron_numbers_in_layers[:-1], \n",
        "                                  neuron_numbers_in_layers[1:])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2-gvLBPdlc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU7teM3C1lDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_tensor(nn,p):\n",
        "  i = 0;\n",
        "  for param in nn.parameters():\n",
        "    k = 0;\n",
        "    for data in param.data:\n",
        "      try:\n",
        "        j = 0;\n",
        "        for d in data:#for [[]]\n",
        "          #print(\"param.data[k][j]\",i,param.data[k][j])\n",
        "          param.data[k][j] = float(p[i])\n",
        "          #print(\"param.data[k][j]\",i,param.data[k][j])\n",
        "          i+=1\n",
        "          j+=1\n",
        "          print(i)\n",
        "        k+=1\n",
        "\n",
        "      except:\n",
        "        try:\n",
        "          j = 0;\n",
        "          for d in data:#for [[[]]]\n",
        "            l = 0;\n",
        "            for d2 in d:\n",
        "              #print(\"param.data[k][j][l]\",param.data[k][j][l])\n",
        "              param.data[k][j][l] = float(p[i])\n",
        "              #print(\"p[i]\",p[i])\n",
        "              i+=1\n",
        "              l+=1\n",
        "              print(i)\n",
        "            j+=1\n",
        "          k += 1\n",
        "        except:\n",
        "          param.data[k] = float(p[i])\n",
        "          i+=1\n",
        "          k+=1;\n",
        "          \n",
        "          \n",
        "def get1DParameters(nn):\n",
        "  p = [];\n",
        "  size = 0;\n",
        "  for param in nn.parameters():\n",
        "    print(param.data)\n",
        "    size+=1\n",
        "    item = param.data.numpy().reshape(-1)\n",
        "    for i in item:\n",
        "      p.append(i)\n",
        "  return p;\n",
        "\n",
        "\n",
        "nn3.eval();\n",
        "from torch.optim import Adam\n",
        "optim = Adam(nn3.parameters(), lr=0.01) # manager: adjust params according to grads\n",
        "for param in nn3.parameters():\n",
        "  print(param.data)\n",
        "\n",
        "\n",
        "p = get1DParameters(nn3);\n",
        "\n",
        "numpyArray = [];\n",
        "p[4] = 66\n",
        "i = 0\n",
        "print(p)\n",
        "      \n",
        "update_tensor(nn3,p);\n",
        "\n",
        "print(\"size:\",len(p))\n",
        "for param in nn3.parameters():\n",
        "  print(param.data)\n",
        "# for param in nn3.parameters():\n",
        "#   numpyArray.append(param.data.numpy())\n",
        "#   i+=1\n",
        "\n",
        "# k = 0\n",
        "# for n in range(len(numpyArray)):\n",
        "#   print(numpyArray[2])\n",
        "#   print(len(numpyArray[n]))\n",
        "#   for m in range(len(numpyArray[n])):\n",
        "#     numpyArray[n][m] = p[k]\n",
        "#     k+=1\n",
        "  \n",
        "# numpyArray = np.asarray(numpyArray)\n",
        "# print(numpyArray)\n",
        "\n",
        "# for i in range(len(numpyArray)):\n",
        "#   print(torch.from_numpy(numpyArray[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}